{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grundlagen neuronaler Netze\n",
    "\n",
    "\n",
    "## Biologischer Hintergrund\n",
    "- Dendriten,\n",
    "- Soma\n",
    "- Axon\n",
    "\n",
    "## Perzeptron\n",
    "- Unterschied zu Neuron?\n",
    "\n",
    "## Aktivierungsfunktionen\n",
    "\n",
    "- Identität\n",
    "- Relu $R(x) = max(0,x)$\n",
    "- Sigmoid $S(x) = 1/(1+e^-x)$\n",
    "- tanh $T(x) = (1-e^-2x)/(1+e^-2x)$\n",
    "- Threshold-Funktion $U(x,t) = \n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t1  & x \\geq t \\\\\n",
    "\t\t0 & else\n",
    "\t\\end{array}\n",
    "\\right.$\n",
    "\n",
    "Jeweils zeichnen mit matplotlib\n",
    "\n",
    "## supervised vs non-supervised learning\n",
    "\n",
    "### supervised learning\n",
    "Daten liegen mit der korrekten Antwort $\\hat{y}$ vor. Man kann überprüfen, wie weit die Ergebnisse y von diesem Wert abweichen.\n",
    "Beispiel:\n",
    "gegeben: $x_1, x_2, x_3, y$\n",
    "\n",
    "### non-supervised learning\n",
    "Daten liegen ohne korrekte Antwort vor. Gewöhnlich werden Ähnlichkeiten in den Daten gesucht, Nähe zu anderen Datenelementen oder es werden häufig auftretende Strukturen stärker gewichtet.\n",
    "\n",
    "Beispiel:\n",
    "gegeben: $x_1, x_2, x_3$\n",
    "\n",
    "### Hebbsche Lernregel\n",
    "Gewichte von Neuronen, die zusammen aktiv sind, werden verstärkt.\n",
    "\n",
    "\"what fires together, wires together\"\n",
    "\n",
    "$\\Delta w_{ij} = \\eta x_i y_j$\n",
    "\n",
    "## Wir basteln uns ein Neuron\n",
    "\n",
    "Eingabe: $x \\in \\mathbb{R}^n$\n",
    "\n",
    "Aufgaben:\n",
    "- Summe bilden\n",
    "- Gewichtete Summe bilden mit $w_i = 1$\n",
    "- Bias hinzufügen\n",
    "- Gewichte mit randomisierten Werte belegen $w$\n",
    "- Aktivierungsfunktion ergänzen\n",
    "- Das was wir hier ausgerechnet haben, nennen wir $y$\n",
    "$y = w_i * x_i + b$\n",
    "\n",
    "## Problem: wie lernt man die \"richtigen\" Gewichte\n",
    "\n",
    "### Was ist \"richtig\"?\n",
    "\n",
    "gegeben:\n",
    "Eingabedaten $x_1, ... x_n$\n",
    "korrekte Antwort $\\hat{y}$\n",
    "\n",
    "Beipiel:\n",
    "\n",
    "$x_i$: Bild mit Löwe (jedes Pixel ist ein $x_i$),\n",
    "$\\hat(y)$: korrekte Antwort: \"Löwe\"\n",
    "\n",
    "Das Netz berechnet $y$: \"Auto\".\n",
    "\n",
    "## Fehlerfunktion\n",
    "Der Fehler ist die Differenz zwischen $y$ und $\\hat{y}$, meist quadriert.\n",
    "\n",
    "$E(x) = |\\hat{y} - y|$\n",
    "\n",
    "oder\n",
    "\n",
    "$E(x) = (\\hat{y} - y)^2$\n",
    "\n",
    "oder Kreuzkorrelation\n",
    "\n",
    "Wie verbessere ich die Gewichte, um den Fehler zu minimieren?\n",
    "\n",
    "### Grundlage: Gradientenabstiegsverfahren Newton-Verfahren\n",
    "\n",
    "- Was macht Newton $x_{n+1} = x_n - f(x_n) / f'(x_n)$\n",
    "- Erklärung des Verfahrens (Algorithmus)\n",
    "- Klarmachen, wann das Verfahren endet\n",
    "- Gutartiges Beispiel \n",
    "    - Nullstelle einer Polynomfunktion mit Python und matplotlib\n",
    "    - Lernerfolg: schrittweises Erreichen der Nullstelle\n",
    "- Beispiel aus https://en.wikipedia.org/wiki/Newton%27s_method \n",
    "    - $y = x^3 − 2x + 2$, Startwert $x_0 = 0$\n",
    "    - Mit oszilierenden Werten bei Newton in Python von den Studis machen lassen\n",
    "    - Lösung: anderer Startwert\n",
    "    - Lernerfolg: klappt nicht immer, daher evtl. andere Verfahren: Laplace-Verfahren, ...\n",
    "\n",
    "### Integration der Bias Gewichte\n",
    "\n",
    "Hinweis: die folgenden Indices zeigen die Anzahl der Elemente, nicht das Element an!\n",
    "\n",
    "$y = w_{ij} * x_i + b_j$\n",
    "\n",
    "$<=> y = w_{ij} * x_i + b_{1j} * 1$\n",
    "\n",
    "Behandeln der $b_{1j}$ als $w_{1j}$\n",
    "\n",
    "$<=> y = w_{ij} * x_i + w_{1j} * x_{i+1}$ wobei $x_{i+1} = 1$\n",
    "\n",
    "$<=> y = w_{(i+1)j} * x_{i+1}$ wobei $x_{i+1} = 1$\n",
    "\n",
    "### Lernen der Gewichte\n",
    "\n",
    "Ein Gradientenabstiegsverfahren kann auf die Fehlerfunktion angewendet werden, um die Gewichte zu verbessern.\n",
    "\n",
    "- Neuen Wert für Gewichte als Summe aus alten Gewichten und Faktor * neue Gewichte\n",
    "\n",
    "$w_{i+1j} = w_{ij} + \\Delta w_{ij}$\n",
    "\n",
    "mit\n",
    "\n",
    "$\\Delta w_{ij} = -\\eta \\dfrac{\\delta E(x)}{\\delta w_{ij}}$\n",
    "\n",
    "\n",
    "### Lernrate $\\Eta$\n",
    "- Die Schrittweite nennt man Lernrate.\n",
    "- Gewöhnlich liegt die Lernrate bei $0.0001 < \\eta < 0.1$.\n",
    "- Die Lernrate kann reduziert werden, um anfänglich mit großen, später mit kleinen Schritten zum Ziel zu kommen.\n",
    "- Was passiert bei $\\alpha = 0$?\n",
    "\n",
    "### Einfaches Beispiel (Zahlen größer 5):\n",
    "- Vector mit Samples anlegen\n",
    "- Vector mit korrekten Ergebnissen anlegen\n",
    "- Ausgabe: [0, 1]\n",
    "- Startwerte $w = 0, b = 0$ und Schrittweite $\\alpha = 0.01$\n",
    "- 1 Schritt manuell durchrechnen\n",
    "    - Beobachtung: y wird besser\n",
    "\n",
    "### Begriff der Epoche\n",
    "- Beispiel mit $epoch = [1, 10, 100,...]$ Epochen durchrechnen und vergleichen\n",
    "    - mehr Epochen => besser, aber irgendwann kaum Änderungen\n",
    "\n",
    "Untersuchung verschiedener Schrittweiten\n",
    "- Startwerte für $w = 0, b = 0$ (für alle Gruppen gleich)\n",
    "- Mehrere Gruppen machen das mit vorgegebenen aber verschiedenen Lernraten $\\eta = [1.0, 0.5, 0.0, 0.1, 0.02, ...]$\n",
    "\n",
    "Untersuchung verschiedener Startwerte\n",
    "- Verschiedene Startwerte für Gruppen mit $w = [-1, 0, 1, random, 10000], b = [0, 1, random, 1000]$\n",
    "- Lernrate $\\eta = 0.01$\n",
    "\n",
    "Wie findet man sinnvolle Schrittweiten?\n",
    "- Zwischen den Epochen anpassen (z.B. halbieren)\n",
    "\n",
    "Backpropagation als Begriff\n",
    "\n",
    "## Online Lernen vs Batchlearning\n",
    "\n",
    "### Online Learning (Stochastisches Lernen)\n",
    "- Nach jedem Forwardpropagationstep wird ein Backwardpropagationstep ausgeführt.\n",
    "- Gewichte \"flackern\" stärker, da jedes x eine Änderung der Gewichte hervorruft. Dadurch hat das Online-Lernen die Fähigkeit besserr aus lokalen Minima zu entkommen.\n",
    "\n",
    "### Batch-Learning\n",
    "- Große Datenmengen: nicht alles auf eeinmal lernen, kleine Stücke auswählen\n",
    "- Testdaten werden nach jedem Batch getestet\n",
    "- Ausgaben nach jedem Batch zeigen Fortschritt an\n",
    "- Spart Rechenzeit, da nicht nach jedem Wert ein Backprop gemacht werden muss\n",
    "- Indikatoren für \"schlechtes\" Netz nach wenigen Batches statt immer alle Daten \n",
    "\n",
    "\n",
    "\n",
    "### Lineare Trennbarkeit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
